---
categories:
  - deepLearning
tags:
  - deepLearning
date: ‘2021-12-08'
mathjax: true
slug: dl-MLP
title: 深度学习基础——多层感知机
---

多层感知机（Multilayer Perceptron, MLP）是最简单的深度网络。本文回顾多层感知机的相关内容及一些基本概念术语。

<!-- more -->

# 多层感知机

## 为什么需要多层感知机

多层感知机是对线性回归的拓展和修正，可以看做是多层线性回归。因为线性回归中一般采用类似Softmax回归的模型结构，Softmax操作将输入直接映射到输出，虽然其本身非线性，但其仿射变换后输入输出为线性对应关系，即隐藏一个假设：**输入输出之间具有单调线性关系**，即特征的增大一定会导致模型输出的单调增大或减小。然而实际情况中，输入输出显然不一定具有单调性，而且特征之间相互会有影响，因此线性回归具有一定局限性。因此，我们引入深度神经网络。

## 什么是多层感知机

以输入、隐藏、输出三层感知机为例，由上节可知，多层感知机是为了克服线性回归的局限性而引入的。线性回归之所以会存在局限的原因是特征之间会有影响。因此我们可以在输入和输出之间虚拟一层作为”二次输入“（隐藏层）。隐藏层的值由输入层决定，通过线性回归的特性可知，隐藏层受输入层中的特征影响，而对于输出层，隐藏层又可以看做是新的输入层，是**考虑了输入层特征之间影响而得到的输入层**，因此，通过隐藏层的加入，三层感知机可以弥补单层线性回归的局限性。因为层与层间输入输出关系类似于人体交错纵横的神经元网络，所以称为神经网络。

需要注意的是，为了考虑所有特征之间的相互影响，最理想情况是每一层都考虑上一层之间所有特征之间相互影响的可能，即层与层之间为全连接结构。然而根据计算复杂度理论可知，全连接的神经网络的复杂度非常非常高，因此在实际使用中，存在实用性与有效性之间的权衡。

## 如何构建多层感知机

本节从数据角度推导如何建立多层感知机。用矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$表示$n$个样本的小批量，其中每个样本都有$d$个输入特征。为简单起见，此处推导单隐藏层的多层感知机，记其隐藏单元个数为$h$，$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出，称为隐藏表示（hidden representation）、隐藏层变量（hidden-layer variable）或者隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接，所以有隐藏层权重$\mathbf{W^{(1)}} \in \mathbb{R^{n \times h}}$ 和隐藏层偏置$\mathbf{b^{(1)}} \in \mathbb{R^{1 \times h}}$以及输出层权重$\mathbf{W^{(2)}} \in \mathbb{R^{h \times q}}$和输出层偏置$\mathbf{b^{(2)}} \in \mathbb{R^{1\times q}}$。

此时，可以计算出输出层的输出$\mathbf{O} \in \mathbb{R^{n\times q}}$为

$$
\mathbf{H} = \mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}}\\
\mathbf{O} = \mathbf{H}\mathbf{W^{(2)}} + \mathbf{b^{(2)}}
$$

接下来证明通过上式得到的输出与直接进行仿射变换得到的输出相等。联立以上两式，消掉隐藏层输出后可得

$$
\begin{aligned}
\mathbf{O} &= (\mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}})\mathbf{W^{(2)}} + \mathbf{b^{(2)}}\\
&= \mathbf{X}\mathbf{W^{(1)}}\mathbf{W^{(2)}}+\mathbf{b^{(1)}}\mathbf{W^{(2)}}+\mathbf{b^{(2)}}  \\
&=   \mathbf{X}\mathbf{W} + \mathbf{b}
\end{aligned}
$$

通过以上证明可以，添加隐藏之后，模型的输入输出与线性回归相同，仍然遵循仿射变换关系！即添加的隐藏层没有任何作用！因此我们需要对隐藏层的输出进行处理，使其不再满足上式中的变换关系，我们采用的处理手段为激活函数（activation function），在隐藏层仿射变换后，对每个隐藏单元应用激活函数进行处理：

$$
\mathbf{H} = \sigma( \mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}} )\\
\mathbf{O} = \mathbf{H}\mathbf{W^{(2)}} + \mathbf{b^{(2)}}
$$

处理后的输出称为活性值（activations）。经过激活函数处理后，因为激活函数一般不满足仿射变换，所以处理后的多层感知机模型整体不再满足仿射变换关系。因为$\mathbf{X}$中的每一行对应一个样本，因此激活函数也按照行的方式作用于输入。



为了提升多层感知机的能力，可以继续堆叠这样的隐藏层，通过与上述过程类似的处理，可以产生效果更好的模型。

## 激活函数

通过上节的证明可以发现，多层感知机区别于线性回归的仿射变换的关键在于激活函数，激活函数一般是非线性的，通过计算加权和加上偏置来确定该神经元是否被激活，他可以将上一层的输入转换为可微分的输出，下面介绍常见的激活函数。

### ReLU函数

ReLu是修正线性单元（Rectified linear unit，ReLu）的简称。

### sigmoid函数



### tanh函数



# 欠拟合&过拟合

# 权重衰减

# 暂退法

# 前向传播、反向传播、计算图

# 数值稳定性和模型初始化

# 环境和分布偏移
