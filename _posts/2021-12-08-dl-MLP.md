---
categories:
  - deepLearning
tags:
  - deepLearning
date: ‘2021-12-08'
mathjax: true
slug: dl-MLP
title: 深度学习基础——多层感知机
---

多层感知机（Multilayer Perceptron, MLP）是最简单的深度网络。本文回顾多层感知机的相关内容及一些基本概念术语。

<!-- more -->

# 多层感知机

## 为什么需要多层感知机

多层感知机是对线性回归的拓展和修正，可以看做是多层线性回归。因为线性回归中一般采用类似Softmax回归的模型结构，Softmax操作将输入直接映射到输出，虽然其本身非线性，但其仿射变换后输入输出为线性对应关系，即隐藏一个假设：**输入输出之间具有单调线性关系**，即特征的增大一定会导致模型输出的单调增大或减小。然而实际情况中，输入输出显然不一定具有单调性，而且特征之间相互会有影响，因此线性回归具有一定局限性。因此，我们引入深度神经网络。

## 什么是多层感知机

以输入、隐藏、输出三层感知机为例，由上节可知，多层感知机是为了克服线性回归的局限性而引入的。线性回归之所以会存在局限的原因是特征之间会有影响。因此我们可以在输入和输出之间虚拟一层作为”二次输入“（隐藏层）。隐藏层的值由输入层决定，通过线性回归的特性可知，隐藏层受输入层中的特征影响，而对于输出层，隐藏层又可以看做是新的输入层，是**考虑了输入层特征之间影响而得到的输入层**，因此，通过隐藏层的加入，三层感知机可以弥补单层线性回归的局限性。因为层与层间输入输出关系类似于人体交错纵横的神经元网络，所以称为神经网络。

需要注意的是，为了考虑所有特征之间的相互影响，最理想情况是每一层都考虑上一层之间所有特征之间相互影响的可能，即层与层之间为全连接结构。然而根据计算复杂度理论可知，全连接的神经网络的复杂度非常非常高，因此在实际使用中，存在实用性与有效性之间的权衡。

## 如何构建多层感知机

本节从数据角度推导如何建立多层感知机。用矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$表示$n$个样本的小批量，其中每个样本都有$d$个输入特征。为简单起见，此处推导单隐藏层的多层感知机，记其隐藏单元个数为$h$，$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出，称为隐藏表示（hidden representation）、隐藏层变量（hidden-layer variable）或者隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接，所以有隐藏层权重$\mathbf{W^{(1)}} \in \mathbb{R^{n \times h}}$ 和隐藏层偏置$\mathbf{b^{(1)}} \in \mathbb{R^{1 \times h}}$以及输出层权重$\mathbf{W^{(2)}} \in \mathbb{R^{h \times q}}$和输出层偏置$\mathbf{b^{(2)}} \in \mathbb{R^{1\times q}}$。

此时，可以计算出输出层的输出$\mathbf{O} \in \mathbb{R^{n\times q}}$为

$$
\mathbf{H} = \mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}}\\
\mathbf{O} = \mathbf{H}\mathbf{W^{(2)}} + \mathbf{b^{(2)}}
$$

接下来证明通过上式得到的输出与直接进行仿射变换得到的输出相等。联立以上两式，消掉隐藏层输出后可得

$$
\begin{aligned}
\mathbf{O} &= (\mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}})\mathbf{W^{(2)}} + \mathbf{b^{(2)}}\\
&= \mathbf{X}\mathbf{W^{(1)}}\mathbf{W^{(2)}}+\mathbf{b^{(1)}}\mathbf{W^{(2)}}+\mathbf{b^{(2)}}  \\
&=   \mathbf{X}\mathbf{W} + \mathbf{b}
\end{aligned}
$$

通过以上证明可以，添加隐藏之后，模型的输入输出与线性回归相同，仍然遵循仿射变换关系！即添加的隐藏层没有任何作用！因此我们需要对隐藏层的输出进行处理，使其不再满足上式中的变换关系，我们采用的处理手段为激活函数（activation function），在隐藏层仿射变换后，对每个隐藏单元应用激活函数进行处理：

$$
\mathbf{H} = \sigma( \mathbf{X}\mathbf{W^{(1)}} + \mathbf{b^{(1)}} )\\
\mathbf{O} = \mathbf{H}\mathbf{W^{(2)}} + \mathbf{b^{(2)}}
$$

处理后的输出称为活性值（activations）。经过激活函数处理后，因为激活函数一般不满足仿射变换，所以处理后的多层感知机模型整体不再满足仿射变换关系。因为$\mathbf{X}$中的每一行对应一个样本，因此激活函数也按照行的方式作用于输入。

为了提升多层感知机的能力，可以继续堆叠这样的隐藏层，通过与上述过程类似的处理，可以产生效果更好的模型。

## 激活函数

通过上节的证明可以发现，多层感知机区别于线性回归的仿射变换的关键在于激活函数，激活函数一般是非线性的，通过计算加权和加上偏置来确定该神经元是否被激活，他可以将上一层的输入转换为可微分的输出，下面介绍常见的激活函数。

### ReLU函数

ReLu是修正线性单元（Rectified linear unit，ReLu）的简称。对于任一给定元素$x$，ReLU函数为该元素与0的最大值，即函数可表示为

$$
\mathbf{ReLU} = max(x,0)
$$

直观上看，ReLU函数将所有负元素的活性值设为0，从而将仿射变换变为非线性变换。而其导数对于负元素为0，正元素为1，即要么让特征保留，要么让特征消失，可以解决神经网络的梯度消失问题。

### sigmoid函数

sigmoid函数的定义为对于任一定义域在$\mathbb{R}$的输入，可以将其变换为区间$(0,1)$之间的输出，因此sigmoid函数通常被称为挤压函数（squashing function）(将任意输入挤压到(0,1)之间)。

$$
sigmoid(x) = \frac{1}{1+\exp (-x)}
$$

从上式可看出，sigmoid的优点是平滑可微，然而其复杂度较高，因此常被用作输出单元的激活（隐藏层的激活用ReLu函数）。sigmoid函数的导数为

$$
\frac{d}{dx}sigmoid(x) = \frac{exp(-x)}{(1+exp(-x))^2}=sigmoid(x)(1-sigmoid(x))
$$

从导数可以看出，当$x\rightarrow0^-$或$x\rightarrow0^+$时，导数值趋近于0.25，$x\rightarrow+\infty$或$x\rightarrow-\infty$时，导数值趋向于0。通过画sigmoid函数的曲线图或者求二阶导数可以看出，函数在0附近接近线性0。

### tanh函数

tanh（双曲正切）函数与sigmoid函数类似，也是将输入压缩到$(-1,1)$之间，不同的是，sigmoid函数关于原点中心对称。

$$
\tanh (x)=\frac{1-\exp (-2 x)}{1+\exp (-2 x)}
$$

其导数为

$$
\frac{d}{d x} \tanh (x)=1-\tanh ^{2}(x)
$$

# 权重衰减

在训练过程中，过拟合是非常常见的现象，可以通过采用更多的训练数据来缓解过拟合，当数据数目难以增加时，可以采用正则化方法缓解过拟合，比如权重衰减。

权重衰减（weight decay）也被称为$L_2$正则化，通过函数与零的距离来衡量函数的复杂度。函数与零的距离可以采用范数来度量。对于线性函数$f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}$，可以采用其权重向量的范数来度量其与零的距离，即采用权重向量的范数可以度量函数复杂性，例如$\left \| \mathbf{w} \right \|^2 $。训练过程中，降低函数的复杂性，也就是要减小权重向量的范数，可以考虑将范数作为惩罚项加入到损失函数中，即原来的训练目标为最小化预测损失，现在改为最小化预测损失和惩罚项之和。

已有的线性函数的损失函数为：

$$
L(\mathbf{w},b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(\mathbf{w}^T\mathbf{x}^{(i)}+b-y^{(i)})^2
$$

式中，$\mathbf{x}^{(i)}$表示样本$i$的特征，$y^{(i)}$表示样本$i$的标签，$(\mathbf{w},b)$是权重和偏置参数。调整后，需要将权重向量的范数加入到损失函数中，具体如何添加来保持预测损失和权重向量范数的平衡未知，所以用常数$\lambda$来描述添加后的范数。为了消去求导后的常数系数，所以常数采用$\frac{\lambda}{2}$的形式。同样地，为了便于计算，采用平方范数进行度量而不是标准范数。易知，$\frac{\lambda}{2}$还是$\lambda$，采用平方范数还是标准范数对损失函数没有影响。添加后的损失函数为

$$
L(\mathbf{w},b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(\mathbf{w}^T\mathbf{x}^{(i)}+b-y^{(i)})^2+\frac{\lambda}{2}\left\|\mathbf{w}\right\|^2
$$

当$\lambda$等于0时，损失函数没有变化。当$\lambda > 0$时，可以对范数起到权衡作用。此外，选择$L_2$范数而不是$L_1$范数的原因是，$L_2$范数对权重向量的大分量施加了巨大的惩罚，这使得训练过程中偏向于大量特征上均匀分布权重的模型，相比之下，$L_1$范数会将权重集中于一小部分特征，将其他特征清除为0，因此适合于特征选择等场景。

# 暂退法

训练模型的目的是为了让模型能在未知的数据上获得较好的表现，即良好的泛化性。统计学中的泛化理论认为，为了模型在训练集和测试集上的性能差距，应该使模型尽可能简单。通过上节的讨论可知，模型的简单性可以通过范数来衡量，它是模型简单性的一种度量，模型的简单性还可以从其他角度度量，比如模型的平滑性，即模型不应对输入的微小变化敏感，如果给训练数据中添加随机噪声，对模型效果的影响应该是有限的。因此，可以考虑在训练有多层的深层网络时，向每一层注入噪声以增强输入输出映射上的平滑性。这种方法被称为暂退法（drop out）。

顾名思义，暂退法是在训练过程中做舍弃（drop out），在整个训练过程的每一次迭代中，将当前层中的一些节点置0。问题的关键在于如何添加这种随机噪声，噪声应满足何种特性。目前常用的一种噪声注入方式是以一种无偏向（unbiased）的方式注入噪声，即噪声满足均值为0的正态分布。这样在固定其他层时，每一层的期望等于不添加噪声的值。对于输入$\mathbf{x}$，在添加服从$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$的正态分布采样噪声后，产生扰动点$\mathbf{x'}=\mathbf{x}+\epsilon$，扰动点的期望为$E[\mathbf{x'}] = e[\mathbf{x}]$。

采用暂退法之后，如果要进行正则化，可以通过按照保留节点的分数进行规范化来进行。即，中间层的活性值$h$以暂退概率$p$替换为$h'$。

$$
h^{\prime}=\left\{\begin{array}{ll}
0 & \text { 概率为 } p \\
\frac{h}{1-p} & \text { 其他情况 }
\end{array}\right.
$$

易知，此时的期望值仍保持不变，仍有$E[h']=h$成立。

需要注意的是，一般在测试时不用暂退法，也有特殊情况：在测试网络预测的不确定性时可以采用暂退法，如果不同的暂退法遮盖后得到的预测结构都一致，则网络较为稳定。

# 前向传播、反向传播、计算图

本节以带权重衰减的单隐藏层多层感知机为例推导前向传播和反向传播。



![](/img/20211218/forward.jpg)

# 数值稳定性和模型初始化

# 环境和分布偏移
